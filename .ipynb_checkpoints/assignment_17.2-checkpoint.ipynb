{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a9aa89c-81ed-4235-8cd2-74841169f1cd",
   "metadata": {},
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bacf97-c6ad-4b45-8f50-6e2f70a1a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, ShuffleSplit, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.decomposition import PCA  \n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from collections import Counter  \n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable \n",
    "\n",
    "from ucimlrepo import fetch_ucirepo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307db5fd-382b-40a5-9ea1-e21179cc744e",
   "metadata": {},
   "source": [
    "### Import and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb35a449-545c-44d1-bcbd-6966fb465b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './'\n",
    "\n",
    "try:\n",
    "    X = pd.read_csv(path+'X.csv')\n",
    "    y = pd.read_csv(path+'y.csv')\n",
    "    print('Data loaded from directory:', path)\n",
    "except:\n",
    "    bank_marketing = fetch_ucirepo(id=222) \n",
    "    X = bank_marketing.data.features \n",
    "    y = bank_marketing.data.targets \n",
    "    print('Data loaded from UCIML')\n",
    "\n",
    "df = X\n",
    "df['y'] = y['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee1137-4e7f-4a41-8a66-b48aefeb792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "percents = (df.isna().sum() / len(X)) * 100\n",
    "formatted_percents = percents.apply(lambda x: f'{x:.2f}%')\n",
    "print(formatted_percents, '\\n')\n",
    "print(f\"Length of DataFrame: {len(df)}\", '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea15d1b-1c2b-4be1-8491-ad5ddc58f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.drop(columns = ['poutcome', 'contact'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1092710b-1c57-4b33-82d2-9d2bc8ba8605",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad215b5e-117b-4ab1-9653-b631ddc41f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df_clean))\n",
    "print(len(df))\n",
    "print(len(df_clean))\n",
    "print(df_clean.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6e73ec-3323-4a61-81f6-3c51ec43fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with data types of each column\n",
    "dtype_info = pd.DataFrame({\n",
    "    'Column': df_clean.columns,\n",
    "    'Dtype': df_clean.dtypes\n",
    "})\n",
    "\n",
    "# Add a new column for the number of unique values if the column is of type 'object'\n",
    "dtype_info['Unique Values'] = dtype_info['Column'].apply(lambda col: df_clean[col].nunique() if df_clean[col].dtype == 'object' else None)\n",
    "dtype_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d7b8e2-3104-4719-96c8-7dbcec40c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df_clean.columns:\n",
    "    if df_clean[c].dtype == 'object' or c in ['day_of_week']:\n",
    "        print(f'{c}: \\n{sorted(df_clean[c].unique())}', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36190408-4b48-4860-8500-f0d61ef323a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correctly rename the column without assigning it back to the DataFrame\n",
    "df_clean.rename(columns={'day_of_week': 'day_of_month'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb65786-5228-43ef-9f5a-25d4734c00bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify object-type columns (typically categorical)\n",
    "object_columns = df_clean.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create a binary column for pdays = -1\n",
    "df_clean['not_previously_contacted'] = (df_clean['pdays'] == -1).astype(int)\n",
    "\n",
    "# One-hot encode only the object columns, dropping the first category\n",
    "df_encoded = pd.get_dummies(df_clean, columns=object_columns, drop_first=True, dtype=float)\n",
    "\n",
    "# Optionally, check the result\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c836bc59-fd50-46af-98e7-b2a66e43fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded.drop(columns='y')\n",
    "y = df_encoded['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297110ba-adb3-43e3-a449-43c57cc2e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcec5c6-031a-477e-9e70-96c8dd347968",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "    0: 'yes',\n",
    "    1: 'no'\n",
    "}\n",
    "\n",
    "class_distribution = y.value_counts(normalize=True) * 100\n",
    "\n",
    "print('Class distribution of training data:')\n",
    "for label, value in class_distribution.items():\n",
    "    print(f\"{class_mapping[label]}: {value:.2f}%\")\n",
    "    \n",
    "print(f'\\nLength of training data: {len(y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd2e311-fab8-4ce7-9171-6cb2784c38d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_scale_pca(X, y, test_size=0.3, random_state=42, n_components=None, verbose=True):\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # PCA Decomposition\n",
    "    if n_components is not None:\n",
    "        pca = PCA(n_components=n_components)\n",
    "    \n",
    "        X_train_final = pca.fit_transform(X_train_scaled)\n",
    "        X_test_final = pca.transform(X_test_scaled)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Reduced dimensions: {X_train_final.shape[1]}\")\n",
    "    else:\n",
    "        X_train_final = X_train_scaled\n",
    "        X_test_final = X_test_scaled\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Train Data Shape: {X_train_final.shape}\")\n",
    "        print(f\"Test Data Shape: {X_test_final.shape}\")\n",
    "\n",
    "    return X_train_final, X_test_final, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355482f2-6b1c-4f80-a898-992966273bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_scale_pca(X, y, test_size=0.3, random_state=42, n_components=2, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841171f7-23df-46ed-bf8b-9390b1ef2acc",
   "metadata": {},
   "source": [
    "## Training and Evaluation - Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fa7c9b-efff-453f-bb7c-929d5958ddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "        'params': {'C': [0.1, 1], 'solver': ['liblinear', 'lbfgs']}\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'model': DecisionTreeClassifier(random_state=42),\n",
    "        'params': {'max_depth': [5, 10], 'criterion': ['gini', 'entropy']}\n",
    "    },\n",
    "    'KNN': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {'n_neighbors': [3, 5], 'weights': ['uniform', 'distance']}\n",
    "    },\n",
    "    'SVC': {\n",
    "        'model': SVC(class_weight='balanced'),\n",
    "        'params': {'kernel': ['linear', 'rbf'], 'C': [0.1, 1]}\n",
    "    }\n",
    "}\n",
    "\n",
    "train_times = {}\n",
    "train_preds = {}\n",
    "train_accuracies = {}\n",
    "test_preds = {}\n",
    "test_accuracies = {}\n",
    "best_models = {}\n",
    "\n",
    "for name, config in param_grids.items():\n",
    "    print(f'Training {name} with GridSearchCV')\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=config['model'],\n",
    "        param_grid=config['params'],\n",
    "        cv=ShuffleSplit(n_splits=1, test_size=0.2, random_state=42), \n",
    "        n_jobs=-1, \n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    avg_train_time = elapsed_time / len(grid_search.cv_results_['params'])\n",
    "    train_times[name] = avg_train_time\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_models[name] = best_model\n",
    "\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "    train_preds[name] = y_train_pred\n",
    "    train_accuracies[name] = accuracy_score(y_train, y_train_pred)\n",
    "    test_preds[name] = y_test_pred\n",
    "    test_accuracies[name] = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    print(f'{name} - Best Params: {grid_search.best_params_}')\n",
    "    print(f'{name} - Train Accuracy: {train_accuracies[name]:.4f}')\n",
    "    print(f'{name} - Test Accuracy: {test_accuracies[name]:.4f}')\n",
    "    print(f'{name} - Average Train Time: {avg_train_time:.4f} seconds\\n')\n",
    "\n",
    "print('Training times for each model:')\n",
    "for name, train_time in train_times.items():\n",
    "    print(f'{name}: {train_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db48e7a0-f600-4747-89da-5cb8d0064870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model names as you defined them\n",
    "model_names = ['Logistic Regression', 'Decision Tree', 'KNN', 'SVC']  \n",
    "\n",
    "# Step 1: Store confusion matrices and calculate the maximum value across all matrices\n",
    "confusion_matrices = {}\n",
    "max_value = 0\n",
    "\n",
    "for name in model_names:\n",
    "    # Calculate the confusion matrix\n",
    "    y_test_pred = test_preds[name]\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    confusion_matrices[name] = cm\n",
    "    max_value = max(max_value, cm.max())  # Update the maximum value for consistent color scaling\n",
    "\n",
    "# Step 2: Set up the 2x2 grid for the confusion matrices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "axes = axes.flatten()  # Flatten the axes array to easily access each subplot\n",
    "\n",
    "# Step 3: Plot each confusion matrix with consistent color scaling\n",
    "for idx, name in enumerate(model_names):\n",
    "    best_model = best_models[name]  \n",
    "    train_accuracy = train_accuracies[name]  \n",
    "    test_accuracy = test_accuracies[name] \n",
    "\n",
    "    # Print the model performance\n",
    "    print(f'{name} Model Performance:')\n",
    "    print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "    print(f\"Testing Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "    # Use the stored test predictions\n",
    "    y_test_pred = test_preds[name]\n",
    "\n",
    "    # Print the classification report\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "    # Retrieve the confusion matrix\n",
    "    cm = confusion_matrices[name]\n",
    "    print(f'Confusion Matrix:\\n{cm}\\n')\n",
    "\n",
    "    # Manually plot the confusion matrix using consistent color scaling\n",
    "    im = axes[idx].imshow(cm, interpolation='nearest', cmap='Blues', vmin=0, vmax=max_value)\n",
    "    axes[idx].set_title(f'Confusion Matrix: {name}')\n",
    "    \n",
    "    # Add labels to the axes\n",
    "    axes[idx].set_xticks([0, 1])\n",
    "    axes[idx].set_yticks([0, 1])\n",
    "    axes[idx].set_xticklabels(['0', '1'])\n",
    "    axes[idx].set_yticklabels(['0', '1'])\n",
    "    axes[idx].set_xlabel('Predicted label')\n",
    "    axes[idx].set_ylabel('True label')\n",
    "\n",
    "    # Annotate each cell with the corresponding count\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            axes[idx].text(j, i, format(cm[i, j], 'd'),\n",
    "                           ha=\"center\", va=\"center\",\n",
    "                           color=\"white\" if cm[i, j] > max_value / 2 else \"black\")  # Dynamic text color\n",
    "\n",
    "    # Use make_axes_locatable to create a color bar that aligns with the matrix\n",
    "    divider = make_axes_locatable(axes[idx])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "    fig.colorbar(im, cax=cax)\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c329307b-28ab-4ccd-91f8-d9fa54e00132",
   "metadata": {},
   "source": [
    "## Training and Evaluation - Resampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763493db-9bca-4a4b-97e8-c9e717331414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameter grids for GridSearchCV\n",
    "# param_grids = {\n",
    "#     'Logistic Regression - Balanced': {\n",
    "#         'model': LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "#         'params': {'C': [0.1, 1], 'solver': ['liblinear', 'lbfgs']}\n",
    "#     },\n",
    "#     'Decision Tree - Balanced': { \n",
    "#         'model': DecisionTreeClassifier(random_state=42),\n",
    "#         'params': {'max_depth': [5, 10], 'criterion': ['gini', 'entropy']}\n",
    "#     },\n",
    "#     'KNN - Balanced': {\n",
    "#         'model': KNeighborsClassifier(),\n",
    "#         'params': {'n_neighbors': [3, 5], 'weights': ['uniform', 'distance']}\n",
    "#     },\n",
    "#     'SVC - Balanced': {\n",
    "#         'model': SVC(class_weight='balanced'),\n",
    "#         'params': {'kernel': ['linear', 'rbf'], 'C': [0.1, 1]}\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Define seeds for each fold\n",
    "# seeds = [43, 44, 45, 46]\n",
    "\n",
    "# # Initialize dictionaries to store results\n",
    "# balanced_train_times = {}\n",
    "# balanced_best_params = {}\n",
    "# balanced_train_accuracies = {name: [] for name in param_grids}\n",
    "# balanced_test_accuracies = {name: [] for name in param_grids}\n",
    "# balanced_best_params_all_folds = {name: [] for name in param_grids}\n",
    "# balanced_best_models = {}\n",
    "# balanced_test_preds = {}  \n",
    "\n",
    "# # Inside the training loop for each seed and model, store the best model\n",
    "# for seed in seeds:\n",
    "#     print(f\"\\n=== Starting Fold with Seed {seed} ===\")\n",
    "    \n",
    "#     # Random under-sampling with the current seed\n",
    "#     undersampler = RandomUnderSampler(random_state=seed)\n",
    "#     X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = split_scale_pca(X_resampled, y_resampled, test_size=0.3, random_state=seed, n_components=2, verbose=False)\n",
    "\n",
    "#     # Iterate over models in param_grids and perform training\n",
    "#     for name, config in param_grids.items():\n",
    "#         print(f'Training {name} with GridSearchCV for seed {seed}')\n",
    "\n",
    "#         # Setup and fit GridSearchCV\n",
    "#         grid_search = GridSearchCV(\n",
    "#             estimator=config['model'],\n",
    "#             param_grid=config['params'],\n",
    "#             cv=ShuffleSplit(n_splits=1, test_size=0.2, random_state=seed),\n",
    "#             n_jobs=-1,\n",
    "#             verbose=1\n",
    "#         )\n",
    "\n",
    "#         # Train the model\n",
    "#         start_time = time.time()\n",
    "#         grid_search.fit(X_train, y_train)\n",
    "#         elapsed_time = time.time() - start_time\n",
    "\n",
    "#         # Calculate average train time\n",
    "#         avg_train_time = elapsed_time / len(grid_search.cv_results_['params'])\n",
    "#         balanced_train_times[f'{name} (Seed {seed})'] = avg_train_time\n",
    "\n",
    "#         # Get the best model from this fold and store it\n",
    "#         best_model = grid_search.best_estimator_\n",
    "#         balanced_best_models[name] = best_model  # Store best model for each algorithm\n",
    "\n",
    "#         # Store best params for this fold\n",
    "#         balanced_best_params_all_folds[name].append(grid_search.best_params_)\n",
    "\n",
    "#         # Train and test predictions\n",
    "#         y_train_pred = best_model.predict(X_train)\n",
    "#         y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "#         # Store accuracies and predictions\n",
    "#         balanced_train_accuracies[name].append(accuracy_score(y_train, y_train_pred))\n",
    "#         balanced_test_accuracies[name].append(accuracy_score(y_test, y_test_pred))\n",
    "        \n",
    "#         # NEW: Store test predictions for evaluation later\n",
    "#         balanced_test_preds[name] = y_test_pred\n",
    "\n",
    "#         # Print results\n",
    "#         print(f'{name} (Seed {seed}) - Best Params: {grid_search.best_params_}')\n",
    "#         print(f'{name} (Seed {seed}) - Train Accuracy: {balanced_train_accuracies[name][-1]:.4f}')\n",
    "#         print(f'{name} (Seed {seed}) - Test Accuracy: {balanced_test_accuracies[name][-1]:.4f}')\n",
    "#         print(f'{name} (Seed {seed}) - Average Train Time: {avg_train_time:.4f} seconds\\n')\n",
    "\n",
    "\n",
    "# # Calculate average accuracies and find the most common best params\n",
    "# for name in param_grids:\n",
    "#     avg_train_acc = sum(balanced_train_accuracies[name]) / len(balanced_train_accuracies[name])\n",
    "#     avg_test_acc = sum(balanced_test_accuracies[name]) / len(balanced_test_accuracies[name])\n",
    "\n",
    "#     # Find the most common best params\n",
    "#     best_params_counter = Counter([str(params) for params in balanced_best_params_all_folds[name]])\n",
    "#     most_common_params = best_params_counter.most_common(1)\n",
    "\n",
    "#     # If there's a tie or no clear winner, use the best params from the highest test accuracy fold\n",
    "#     if most_common_params[0][1] == 1:\n",
    "#         best_idx = balanced_test_accuracies[name].index(max(balanced_test_accuracies[name]))\n",
    "#         best_params_for_model = balanced_best_params_all_folds[name][best_idx]\n",
    "#     else:\n",
    "#         best_params_for_model = eval(most_common_params[0][0])\n",
    "\n",
    "#     print(f'\\n=== Summary for {name} ===')\n",
    "#     print(f'Average Train Accuracy: {avg_train_acc:.4f}')\n",
    "#     print(f'Average Test Accuracy: {avg_test_acc:.4f}')\n",
    "#     print(f'Most Common Best Params: {best_params_for_model}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6648d71c-df8c-430b-8f4f-46aec2280cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grids for GridSearchCV\n",
    "param_grids = {\n",
    "    'Logistic Regression - Resampled': {\n",
    "        'model': LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "        'params': {'C': [0.1, 1], 'solver': ['liblinear', 'lbfgs']}\n",
    "    },\n",
    "    'Decision Tree - Resampled': { \n",
    "        'model': DecisionTreeClassifier(random_state=42),\n",
    "        'params': {'max_depth': [5, 10], 'criterion': ['gini', 'entropy']}\n",
    "    },\n",
    "    'KNN - Resampled': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {'n_neighbors': [3, 5], 'weights': ['uniform', 'distance']}\n",
    "    },\n",
    "    'SVC - Resampled': {\n",
    "        'model': SVC(class_weight='balanced'),\n",
    "        'params': {'kernel': ['linear', 'rbf'], 'C': [0.1, 1]}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define seeds for each fold\n",
    "seeds = [43, 44, 45, 46]\n",
    "\n",
    "# Initialize dictionaries to store results\n",
    "resampled_train_times = {}\n",
    "resampled_best_params = {}\n",
    "resampled_train_accuracies = {name: [] for name in param_grids}\n",
    "resampled_test_accuracies = {name: [] for name in param_grids}\n",
    "resampled_best_params_all_folds = {name: [] for name in param_grids}\n",
    "resampled_best_models = {}\n",
    "resampled_test_preds = {}  \n",
    "\n",
    "# Inside the training loop for each seed and model, store the best model\n",
    "for seed in seeds:\n",
    "    print(f\"\\n=== Starting Fold with Seed {seed} ===\")\n",
    "    \n",
    "    # Random under-sampling with the current seed\n",
    "    undersampler = RandomUnderSampler(random_state=seed)\n",
    "    X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = split_scale_pca(X_resampled, y_resampled, test_size=0.3, random_state=seed, n_components=2, verbose=False)\n",
    "\n",
    "    # Iterate over models in param_grids and perform training\n",
    "    for name, config in param_grids.items():\n",
    "        print(f'Training {name} with GridSearchCV for seed {seed}')\n",
    "\n",
    "        # Setup and fit GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=config['model'],\n",
    "            param_grid=config['params'],\n",
    "            cv=ShuffleSplit(n_splits=1, test_size=0.2, random_state=seed),\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        start_time = time.time()\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        # Calculate average train time\n",
    "        avg_train_time = elapsed_time / len(grid_search.cv_results_['params'])\n",
    "        resampled_train_times[f'{name} (Seed {seed})'] = avg_train_time\n",
    "\n",
    "        # Get the best model from this fold and store it\n",
    "        best_model = grid_search.best_estimator_\n",
    "        resampled_best_models[name] = best_model  # Store best model for each algorithm\n",
    "\n",
    "        # Store best params for this fold\n",
    "        resampled_best_params_all_folds[name].append(grid_search.best_params_)\n",
    "\n",
    "        # Train and test predictions\n",
    "        y_train_pred = best_model.predict(X_train)\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "        # Store accuracies and predictions\n",
    "        resampled_train_accuracies[name].append(accuracy_score(y_train, y_train_pred))\n",
    "        resampled_test_accuracies[name].append(accuracy_score(y_test, y_test_pred))\n",
    "        \n",
    "        # Store test predictions for evaluation later\n",
    "        resampled_test_preds[name] = y_test_pred\n",
    "\n",
    "        # Print results\n",
    "        print(f'{name} (Seed {seed}) - Best Params: {grid_search.best_params_}')\n",
    "        print(f'{name} (Seed {seed}) - Train Accuracy: {resampled_train_accuracies[name][-1]:.4f}')\n",
    "        print(f'{name} (Seed {seed}) - Test Accuracy: {resampled_test_accuracies[name][-1]:.4f}')\n",
    "        print(f'{name} (Seed {seed}) - Average Train Time: {avg_train_time:.4f} seconds\\n')\n",
    "\n",
    "# Calculate average accuracies and find the most common best params\n",
    "for name in param_grids:\n",
    "    avg_train_acc = sum(resampled_train_accuracies[name]) / len(resampled_train_accuracies[name])\n",
    "    avg_test_acc = sum(resampled_test_accuracies[name]) / len(resampled_test_accuracies[name])\n",
    "\n",
    "    # Find the most common best params\n",
    "    best_params_counter = Counter([str(params) for params in resampled_best_params_all_folds[name]])\n",
    "    most_common_params = best_params_counter.most_common(1)\n",
    "\n",
    "    # If there's a tie or no clear winner, use the best params from the highest test accuracy fold\n",
    "    if most_common_params[0][1] == 1:\n",
    "        best_idx = resampled_test_accuracies[name].index(max(resampled_test_accuracies[name]))\n",
    "        best_params_for_model = resampled_best_params_all_folds[name][best_idx]\n",
    "    else:\n",
    "        best_params_for_model = eval(most_common_params[0][0])\n",
    "\n",
    "    print(f'\\n=== Summary for {name} ===')\n",
    "    print(f'Average Train Accuracy: {avg_train_acc:.4f}')\n",
    "    print(f'Average Test Accuracy: {avg_test_acc:.4f}')\n",
    "    print(f'Most Common Best Params: {best_params_for_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3932efb9-7f75-416b-bfe0-6a0f3ce1766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model names with adjusted formatting\n",
    "# model_names = ['Logistic Regression\\n(Balanced)', 'Decision Tree\\n(Balanced)', 'KNN\\n(Balanced)', 'SVC\\n(Balanced)']\n",
    "\n",
    "# # Re-splitting and scaling the dataset\n",
    "# X_train, X_test, y_train, y_test = split_scale_pca(X, y, test_size=0.3, random_state=42, n_components=2, verbose=False)\n",
    "\n",
    "# # Step 1: Store confusion matrices for each model\n",
    "# confusion_matrices = {}\n",
    "# test_accuracies = {}\n",
    "\n",
    "# # Calculate the maximum value across all confusion matrices to set consistent bounds\n",
    "# max_value = 0\n",
    "\n",
    "# for name in model_names:\n",
    "#     # Fetch the best model from the dictionary\n",
    "#     best_model = balanced_best_models[name.replace('\\n(Balanced)', ' - Balanced')]\n",
    "\n",
    "#     # Predict on the test set and store the predictions\n",
    "#     y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "#     # Store the confusion matrix\n",
    "#     cm = confusion_matrix(y_test, y_test_pred)\n",
    "#     confusion_matrices[name] = cm\n",
    "\n",
    "#     # Store the test accuracy\n",
    "#     test_accuracies[name] = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "#     # Update max_value with the highest value found in the confusion matrix\n",
    "#     max_value = max(max_value, cm.max())\n",
    "\n",
    "# # Step 2: Plot the stored confusion matrices using consistent scaling\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "# axes = axes.flatten()  # Flatten the axes array to easily access each subplot\n",
    "\n",
    "# # Iterate through each model, print performance, and plot the confusion matrix\n",
    "# for idx, name in enumerate(model_names):\n",
    "#     cm = confusion_matrices[name]  # Use the stored confusion matrix\n",
    "#     test_accuracy = test_accuracies[name]  # Use the stored test accuracy\n",
    "\n",
    "#     # Clean the title for printing by removing the newline character\n",
    "#     name_cleaned = name.replace('\\n', ' ')\n",
    "\n",
    "#     # Print the accuracy and classification report\n",
    "#     print(f'{name_cleaned} Model Performance:')\n",
    "#     print(f\"Testing Accuracy on Full Test Set: {test_accuracy:.2f}\")\n",
    "#     print(f'Confusion Matrix:\\n{cm}\\n')\n",
    "\n",
    "#     # Manually plot the confusion matrix using consistent color scaling\n",
    "#     im = axes[idx].imshow(cm, interpolation='nearest', cmap='Blues', vmin=0, vmax=max_value)  # Set consistent color scaling\n",
    "\n",
    "#     # Annotate the confusion matrix cells with the text (count values)\n",
    "#     for i in range(cm.shape[0]):\n",
    "#         for j in range(cm.shape[1]):\n",
    "#             axes[idx].text(j, i, format(cm[i, j], 'd'),\n",
    "#                            ha=\"center\", va=\"center\",\n",
    "#                            color=\"white\" if cm[i, j] > max_value / 2 else \"black\")  # Dynamic text color\n",
    "\n",
    "#     # Set the title and labels\n",
    "#     axes[idx].set_title(f'Confusion Matrix: {name}')\n",
    "#     axes[idx].set_xticks([0, 1])\n",
    "#     axes[idx].set_yticks([0, 1])\n",
    "#     axes[idx].set_xticklabels(['0', '1'])\n",
    "#     axes[idx].set_yticklabels(['0', '1'])\n",
    "#     axes[idx].set_xlabel('Predicted label')\n",
    "#     axes[idx].set_ylabel('True label')\n",
    "\n",
    "#     # Use make_axes_locatable to create a color bar that aligns with the matrix\n",
    "#     divider = make_axes_locatable(axes[idx])\n",
    "#     cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "#     fig.colorbar(im, cax=cax)\n",
    "\n",
    "# # Adjust layout for better spacing\n",
    "# plt.tight_layout()\n",
    "# plt.subplots_adjust(top=0.9, hspace=0.4)  # Adjust 'top' for more space at the top, and increase 'hspace'\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a051f7ef-a2fb-4260-806e-f14cc0a66d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model names with adjusted formatting\n",
    "model_names = ['Logistic Regression\\n(Resampled)', 'Decision Tree\\n(Resampled)', 'KNN\\n(Resampled)', 'SVC\\n(Resampled)']\n",
    "\n",
    "# Re-splitting and scaling the dataset\n",
    "X_train, X_test, y_train, y_test = split_scale_pca(X, y, test_size=0.3, random_state=42, n_components=2, verbose=False)\n",
    "\n",
    "# Step 1: Store confusion matrices for each model\n",
    "confusion_matrices = {}\n",
    "test_accuracies = {}\n",
    "\n",
    "# Calculate the maximum value across all confusion matrices to set consistent bounds\n",
    "max_value = 0\n",
    "\n",
    "for name in model_names:\n",
    "    # Fetch the best model from the dictionary\n",
    "    best_model = resampled_best_models[name.replace('\\n(Resampled)', ' - Resampled')]\n",
    "\n",
    "    # Predict on the test set and store the predictions\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Store the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    confusion_matrices[name] = cm\n",
    "\n",
    "    # Store the test accuracy\n",
    "    test_accuracies[name] = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    # Update max_value with the highest value found in the confusion matrix\n",
    "    max_value = max(max_value, cm.max())\n",
    "\n",
    "# Step 2: Plot the stored confusion matrices using consistent scaling\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "axes = axes.flatten()  # Flatten the axes array to easily access each subplot\n",
    "\n",
    "# Iterate through each model, print performance, and plot the confusion matrix\n",
    "for idx, name in enumerate(model_names):\n",
    "    cm = confusion_matrices[name]  # Use the stored confusion matrix\n",
    "    test_accuracy = test_accuracies[name]  # Use the stored test accuracy\n",
    "\n",
    "    # Clean the title for printing by removing the newline character\n",
    "    name_cleaned = name.replace('\\n', ' ')\n",
    "\n",
    "    # Print the accuracy and classification report\n",
    "    print(f'{name_cleaned} Model Performance:')\n",
    "    print(f\"Testing Accuracy on Full Test Set: {test_accuracy:.2f}\")\n",
    "    print(f'Confusion Matrix:\\n{cm}\\n')\n",
    "\n",
    "    # Manually plot the confusion matrix using consistent color scaling\n",
    "    im = axes[idx].imshow(cm, interpolation='nearest', cmap='Blues', vmin=0, vmax=max_value)  # Set consistent color scaling\n",
    "\n",
    "    # Annotate the confusion matrix cells with the text (count values)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            axes[idx].text(j, i, format(cm[i, j], 'd'),\n",
    "                           ha=\"center\", va=\"center\",\n",
    "                           color=\"white\" if cm[i, j] > max_value / 2 else \"black\")  # Dynamic text color\n",
    "\n",
    "    # Set the title and labels\n",
    "    axes[idx].set_title(f'Confusion Matrix: {name}')\n",
    "    axes[idx].set_xticks([0, 1])\n",
    "    axes[idx].set_yticks([0, 1])\n",
    "    axes[idx].set_xticklabels(['0', '1'])\n",
    "    axes[idx].set_yticklabels(['0', '1'])\n",
    "    axes[idx].set_xlabel('Predicted label')\n",
    "    axes[idx].set_ylabel('True label')\n",
    "\n",
    "    # Use make_axes_locatable to create a color bar that aligns with the matrix\n",
    "    divider = make_axes_locatable(axes[idx])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "    fig.colorbar(im, cax=cax)\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9, hspace=0.4)  # Adjust 'top' for more space at the top, and increase 'hspace'\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe04fb2f-ea39-45e8-96a8-3fc48d444613",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba71f250-4e1b-4c62-b78e-24bbc6be4c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grids for GridSearchCV\n",
    "param_grids = {\n",
    "    'Logistic Regression - SMOTE': {\n",
    "        'model': LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "        'params': {'C': [0.1, 1], 'solver': ['liblinear', 'lbfgs']}\n",
    "    },\n",
    "    'Decision Tree - SMOTE': { \n",
    "        'model': DecisionTreeClassifier(random_state=42),\n",
    "        'params': {'max_depth': [5, 10], 'criterion': ['gini', 'entropy']}\n",
    "    },\n",
    "    'KNN - SMOTE': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {'n_neighbors': [3, 5], 'weights': ['uniform', 'distance']}\n",
    "    },\n",
    "    'SVC - SMOTE': {\n",
    "        'model': SVC(class_weight='balanced'),\n",
    "        'params': {'kernel': ['linear', 'rbf'], 'C': [0.1, 1]}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define seeds for each fold\n",
    "seeds = [43, 44, 45, 46]\n",
    "\n",
    "# Initialize dictionaries to store results\n",
    "smote_train_times = {}\n",
    "smote_best_params = {}\n",
    "smote_train_accuracies = {name: [] for name in param_grids}\n",
    "smote_test_accuracies = {name: [] for name in param_grids}\n",
    "smote_best_params_all_folds = {name: [] for name in param_grids}\n",
    "smote_best_models = {}\n",
    "smote_test_preds = {}  \n",
    "\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"\\n=== Starting Fold with Seed {seed} ===\")\n",
    "    \n",
    "    # Define StratifiedKFold for cross-validation\n",
    "    stratified_cv = StratifiedKFold(n_splits=1, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Split original data into train and test\n",
    "    X_train, X_test, y_train, y_test = split_scale_pca(X, y, test_size=0.3, random_state=seed, n_components=2, verbose=False)\n",
    "    \n",
    "    # Apply SMOTE only to the training set\n",
    "    smote = SMOTE(random_state=seed)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    for name, config in param_grids.items():\n",
    "        print(f'Training {name} with GridSearchCV for seed {seed}')\n",
    "\n",
    "        # Setup and fit GridSearchCV with StratifiedKFold\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=config['model'],\n",
    "            param_grid=config['params'],\n",
    "            cv=stratified_cv,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Train the model on the resampled training data\n",
    "        start_time = time.time()\n",
    "        grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        # Calculate average train time\n",
    "        avg_train_time = elapsed_time / len(grid_search.cv_results_['params'])\n",
    "        smote_train_times[f'{name} (Seed {seed})'] = avg_train_time\n",
    "\n",
    "        # Get the best model from this fold and store it\n",
    "        best_model = grid_search.best_estimator_\n",
    "        smote_best_models[name] = best_model  # Store best model for each algorithm\n",
    "\n",
    "        # Store best params for this fold\n",
    "        smote_best_params_all_folds[name].append(grid_search.best_params_)\n",
    "\n",
    "        # Train and test predictions\n",
    "        y_train_pred = best_model.predict(X_train)\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "        # Store accuracies and predictions\n",
    "        smote_train_accuracies[name].append(accuracy_score(y_train, y_train_pred))\n",
    "        smote_test_accuracies[name].append(accuracy_score(y_test, y_test_pred))\n",
    "        \n",
    "        # NEW: Store test predictions for evaluation later\n",
    "        smote_test_preds[name] = y_test_pred\n",
    "\n",
    "        # Print results\n",
    "        print(f'{name} (Seed {seed}) - Best Params: {grid_search.best_params_}')\n",
    "        print(f'{name} (Seed {seed}) - Train Accuracy: {smote_train_accuracies[name][-1]:.4f}')\n",
    "        print(f'{name} (Seed {seed}) - Test Accuracy: {smote_test_accuracies[name][-1]:.4f}')\n",
    "        print(f'{name} (Seed {seed}) - Average Train Time: {avg_train_time:.4f} seconds\\n')\n",
    "\n",
    "\n",
    "# Calculate average accuracies and find the most common best params\n",
    "for name in param_grids:\n",
    "    avg_train_acc = sum(smote_train_accuracies[name]) / len(smote_train_accuracies[name])\n",
    "    avg_test_acc = sum(smote_test_accuracies[name]) / len(smote_test_accuracies[name])\n",
    "\n",
    "    # Find the most common best params\n",
    "    best_params_counter = Counter([str(params) for params in smote_best_params_all_folds[name]])\n",
    "    most_common_params = best_params_counter.most_common(1)\n",
    "\n",
    "    # If there's a tie or no clear winner, use the best params from the highest test accuracy fold\n",
    "    if most_common_params[0][1] == 1:\n",
    "        best_idx = smote_test_accuracies[name].index(max(smote_test_accuracies[name]))\n",
    "        best_params_for_model = smote_best_params_all_folds[name][best_idx]\n",
    "    else:\n",
    "        best_params_for_model = eval(most_common_params[0][0])\n",
    "\n",
    "    print(f'\\n=== Summary for {name} ===')\n",
    "    print(f'Average Train Accuracy: {avg_train_acc:.4f}')\n",
    "    print(f'Average Test Accuracy: {avg_test_acc:.4f}')\n",
    "    print(f'Most Common Best Params: {best_params_for_model}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d4d62d-7366-4c79-872b-21878e93fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Model names with adjusted formatting for SMOTE\n",
    "model_names = ['Logistic Regression\\n(SMOTE)', 'Decision Tree\\n(SMOTE)', 'KNN\\n(SMOTE)', 'SVC\\n(SMOTE)']\n",
    "\n",
    "# Re-splitting and scaling the dataset\n",
    "X_train, X_test, y_train, y_test = split_scale_pca(X, y, test_size=0.3, random_state=42, n_components=2, verbose=False)\n",
    "\n",
    "# Step 1: Store confusion matrices for each model\n",
    "confusion_matrices = {}\n",
    "test_accuracies = {}\n",
    "\n",
    "# Calculate the maximum value across all confusion matrices to set consistent bounds\n",
    "max_value = 0\n",
    "\n",
    "for name in model_names:\n",
    "    # Fetch the best model from the dictionary\n",
    "    best_model = smote_best_models[name.replace('\\n(SMOTE)', ' - SMOTE')]\n",
    "\n",
    "    # Predict on the test set and store the predictions\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Store the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    confusion_matrices[name] = cm\n",
    "\n",
    "    # Store the test accuracy\n",
    "    test_accuracies[name] = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    # Update max_value with the highest value found in the confusion matrix\n",
    "    max_value = max(max_value, cm.max())\n",
    "\n",
    "# Step 2: Plot the stored confusion matrices using consistent scaling\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "axes = axes.flatten()  # Flatten the axes array to easily access each subplot\n",
    "\n",
    "# Iterate through each model, print performance, and plot the confusion matrix\n",
    "for idx, name in enumerate(model_names):\n",
    "    cm = confusion_matrices[name]  # Use the stored confusion matrix\n",
    "    test_accuracy = test_accuracies[name]  # Use the stored test accuracy\n",
    "\n",
    "    # Clean the title for printing by removing the newline character\n",
    "    name_cleaned = name.replace('\\n', ' ')\n",
    "\n",
    "    # Print the accuracy and classification report\n",
    "    print(f'{name_cleaned} Model Performance:')\n",
    "    print(f\"Testing Accuracy on Full Test Set: {test_accuracy:.2f}\")\n",
    "    print(f'Confusion Matrix:\\n{cm}\\n')\n",
    "\n",
    "    # Manually plot the confusion matrix using consistent color scaling\n",
    "    im = axes[idx].imshow(cm, interpolation='nearest', cmap='Blues', vmin=0, vmax=max_value)  # Set consistent color scaling\n",
    "\n",
    "    # Annotate the confusion matrix cells with the text (count values)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            axes[idx].text(j, i, format(cm[i, j], 'd'),\n",
    "                           ha=\"center\", va=\"center\",\n",
    "                           color=\"white\" if cm[i, j] > max_value / 2 else \"black\")  # Dynamic text color\n",
    "\n",
    "    # Set the title and labels\n",
    "    axes[idx].set_title(f'Confusion Matrix: {name}')\n",
    "    axes[idx].set_xticks([0, 1])\n",
    "    axes[idx].set_yticks([0, 1])\n",
    "    axes[idx].set_xticklabels(['0', '1'])\n",
    "    axes[idx].set_yticklabels(['0', '1'])\n",
    "    axes[idx].set_xlabel('Predicted label')\n",
    "    axes[idx].set_ylabel('True label')\n",
    "\n",
    "    # Use make_axes_locatable to create a color bar that aligns with the matrix\n",
    "    divider = make_axes_locatable(axes[idx])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "    fig.colorbar(im, cax=cax)\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9, hspace=0.4)  # Adjust 'top' for more space at the top, and increase 'hspace'\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

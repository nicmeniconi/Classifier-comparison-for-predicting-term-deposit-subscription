# Classification Model Evaluation

This project evaluates various classification models on different datasets using multiple resampling techniques. The aim is to compare model performance in terms of accuracy, precision, recall, F1-score, and computational efficiency (training and testing times). The resampling techniques used include Random Undersampling and SMOTE (Synthetic Minority Over-sampling Technique).

## Summary of Findings

### Full Dataset (Without Resampling)
- **KNN and Decision Tree**: Both models achieved high accuracy (89.00%), with Decision Tree slightly outperforming KNN in training time.
- **SVM**: Exhibited high precision (90.16%) but took considerably longer to train and test than other models.
- **Logistic Regression**: Had the lowest accuracy (74.57%) but was the simplest to interpret and had the fastest training time.

### Resampled Dataset (Random Undersampling)
- **SVM and Decision Tree**: Outperformed Logistic Regression and KNN, with SVM achieving the highest accuracy (75.27%).
- **Training Times**: Were reduced for all models due to the smaller dataset generated by undersampling.

### SMOTE
- **KNN**: Achieved the highest accuracy (84.16%) and F1-score (85.55%), benefiting from the addition of synthetic data.
- **SVM**: Showed good performance with an accuracy of 83.08% and the highest precision (90.55%), but had the longest training times.
- **Improved Recall**: All models showed better recall than the original dataset, indicating more effective identification of the minority class.

### Key Takeaways
- **KNN and SVM**: Performed well across resampling techniques, though SVM required significantly longer training, especially with SMOTE.
- **Decision Tree**: Offered a good balance between accuracy and training time, suitable for scenarios needing quick predictions and interpretability.
- **Logistic Regression**: Served as a useful baseline, with the fastest training time across datasets.

## Performance Metrics

### Accuracy (%)
| Training_Type         | Full    | Resampled | SMOTE   |
|-----------------------|---------|-----------|---------|
| Logistic Regression   | 74.57   | 69.43     | 75.42   |
| Decision Tree         | 88.29   | 71.92     | 79.74   |
| KNN                   | 89.00   | 72.82     | 84.16   |
| SVC                   | 81.87   | 75.27     | 83.08   |

### Precision (%)
| Training_Type         | Full    | Resampled | SMOTE   |
|-----------------------|---------|-----------|---------|
| Logistic Regression   | 87.04   | 69.48     | 86.99   |
| Decision Tree         | 84.19   | 71.93     | 88.85   |
| KNN                   | 86.95   | 72.92     | 87.66   |
| SVC                   | 90.16   | 75.54     | 90.55   |

### Recall (%)
| Training_Type         | Full    | Resampled | SMOTE   |
|-----------------------|---------|-----------|---------|
| Logistic Regression   | 74.57   | 69.43     | 75.42   |
| Decision Tree         | 88.29   | 71.92     | 79.74   |
| KNN                   | 89.00   | 72.82     | 84.16   |
| SVC                   | 81.87   | 75.27     | 83.08   |

### F1-Score (%)
| Training_Type         | Full    | Resampled | SMOTE   |
|-----------------------|---------|-----------|---------|
| Logistic Regression   | 78.71   | 69.41     | 79.32   |
| Decision Tree         | 84.27   | 71.92     | 82.73   |
| KNN                   | 87.38   | 72.79     | 85.55   |
| SVC                   | 84.48   | 75.21     | 85.43   |

### Average Train Time (Seconds)
| Training_Type         | Full    | Resampled | SMOTE   |
|-----------------------|---------|-----------|---------|
| Logistic Regression   | 0.93    | 0.01      | 0.11    |
| Decision Tree         | 0.74    | 0.07      | 0.91    |
| KNN                   | 0.29    | 0.07      | 0.53    |
| SVC                   | 27.52   | 1.70      | 93.34   |

### Test Time (Seconds)
| Training_Type         | Full    | Resampled | SMOTE   |
|-----------------------|---------|-----------|---------|
| Logistic Regression   | 0.00    | 0.00      | 0.00    |
| Decision Tree         | 0.00    | 0.00      | 0.00    |
| KNN                   | 0.90    | 0.14      | 1.02    |
| SVC                   | 25.15   | 2.02      | 39.77   |

## Next Steps
- **Hyperparameter Tuning**: Explore hyperparameter optimization to improve model performance.
- **Ensemble Methods**: Try ensemble techniques like Random Forest or Gradient Boosting.
- **Dimensionality Reduction**: Use techniques like PCA to reduce computational costs.
- **Other Resampling Techniques**: Consider additional methods such as ADASYN or Tomek Links to address class imbalance.

## References
- **Imbalanced-learn Library Documentation**: [https://imbalanced-learn.org/](https://imbalanced-learn.org/)
- **SMOTE: Synthetic Minority Over-sampling Technique Paper**: Chawla et al. (2002). [https://www.jair.org/index.php/jair/article/view/10302](https://www.jair.org/index.php/jair/article/view/10302)
- **Scikit-Learn Library Documentation**: [https://scikit-learn.org/](https://scikit-learn.org/)